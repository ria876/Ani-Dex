{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL \n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import math\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd() # get the path of this current file\n",
    "print(current_path)\n",
    "image_path = current_path.removesuffix(\"OmniModel\") + \"\\\\animals\\\\Cattle Breeds\" # prefix it to the path of the images\n",
    "image_dir = pathlib.Path(image_path).with_suffix('') # get that path as a diretory.\n",
    "print(image_dir.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "image_count = len(list(image_dir.glob(\"*/*\"))) #\n",
    "print(image_count)\n",
    "animal_imgs = {}\n",
    "#choosen_animals = [\"bird\",\"butterfly\",\"cat\",\"dog\"]\n",
    "\n",
    "#for animal in [dir for dir in list(image_dir.iterdir()) if dir.name in choosen_animals]:\n",
    "for animal in list(image_dir.iterdir()):\n",
    "    animal_count = len(list(animal.glob(\"*\")))\n",
    "    animal_name = animal.name\n",
    "    animal_imgs[animal_name] = list(animal.glob(\"*.jpg\")) + list(animal.glob(\"*.jpeg\")) + list(animal.glob(\"*.png\"))\n",
    "    \n",
    "    print(f\"Number of {animal_name} is {animal_count}, {animal_count - len(animal_imgs[animal_name])} not stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ratio = 0.1\n",
    "data_cap = 1000//(len(animal_imgs.keys()))\n",
    "project_folder = \"CattleGroup\"\n",
    "\n",
    "training_path = current_path + \"\\\\\"+ project_folder + \"\\\\TrainingData\"\n",
    "testing_path = current_path + \"\\\\\"+ project_folder + \"\\\\TestingData\"\n",
    "\n",
    "os.makedirs(training_path,exist_ok=True)\n",
    "os.makedirs(testing_path,exist_ok=True)\n",
    "\n",
    "for animal_dir in list(image_dir.iterdir()):\n",
    "    print(\"Gathering data for\",animal_dir.name)\n",
    "    tr_des_path = training_path + \"\\\\\" + animal_dir.name\n",
    "    ts_des_path = testing_path + \"\\\\\" + animal_dir.name\n",
    "    \n",
    "    os.makedirs(tr_des_path,exist_ok=True)\n",
    "    os.makedirs(ts_des_path,exist_ok=True)\n",
    "    \n",
    "    tr_dir = pathlib.Path(tr_des_path).with_suffix('')\n",
    "    ts_dir = pathlib.Path(ts_des_path).with_suffix('')\n",
    "    \n",
    "    tr_size = len(list(tr_dir.iterdir()))\n",
    "    ts_size = len(list(ts_dir.iterdir()))\n",
    "    \n",
    "    print(\"\\tTraining started with\",tr_size)\n",
    "    print(\"\\tTesting started with\",ts_size)\n",
    "    \n",
    "    isfull = {\"training\":False,\"testing\":False}\n",
    "    into_testing = False\n",
    "    while len(animal_imgs[animal_dir.name]) > 0:\n",
    "        isfull[\"training\"] = tr_size >= (1-testing_ratio)*data_cap        \n",
    "        isfull[\"testing\"] = ts_size >= (testing_ratio)*data_cap\n",
    "        \n",
    "        if all(isfull.values()): break\n",
    "        \n",
    "        index = random.randrange(0,len(animal_imgs[animal_dir.name])) \n",
    "        img = animal_imgs[animal_dir.name].pop(index)\n",
    "        try:\n",
    "            full_path = str(img.absolute())\n",
    "            full_path.encode(\"latin-1\")\n",
    "        except UnicodeEncodeError as e:\n",
    "            print(\"Encoding error in file:\",full_path,\"\\n\",e)\n",
    "            os.unlink(full_path)\n",
    "            continue\n",
    "        \n",
    "        if isfull[\"testing\"] and not isfull[\"training\"]: shutil.copy2(full_path,str(tr_dir.absolute()))\n",
    "        elif not isfull[\"testing\"] and isfull[\"training\"]: shutil.copy2(full_path,str(ts_dir.absolute()))\n",
    "        elif not isfull[\"training\"] and not into_testing: shutil.copy2(full_path,str(tr_dir.absolute()))\n",
    "        elif not isfull[\"testing\"] and into_testing: shutil.copy2(full_path,str(ts_dir.absolute()))\n",
    "        into_testing = not(into_testing)\n",
    "        \n",
    "        tr_size = len(list(tr_dir.iterdir()))\n",
    "        ts_size = len(list(ts_dir.iterdir()))\n",
    "    \n",
    "    print(\"\\tTraining ended with\",tr_size)\n",
    "    print(\"\\tTesting ended with\",ts_size)               \n",
    "    print()\n",
    "\n",
    "print(\"finished\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = pathlib.Path(training_path).with_suffix('')\n",
    "testing_dir = pathlib.Path(testing_path).with_suffix('')\n",
    "\n",
    "print(\"Training Info:\")\n",
    "print(\"Dir:\",training_dir.absolute())\n",
    "image_count = len(list(training_dir.glob(\"*/*\")))\n",
    "print(image_count)\n",
    "for animal in list(training_dir.iterdir()):\n",
    "    animal_count = len(list(animal.glob(\"*\")))\n",
    "    animal_name = animal.name\n",
    "    print(f\"Number of {animal_name} is {animal_count}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Testing Info: \")\n",
    "print(\"Dir:\",testing_dir.absolute())\n",
    "image_count = len(list(testing_dir.glob(\"*/*\")))\n",
    "print(image_count)\n",
    "for animal in list(testing_dir.iterdir()):\n",
    "    animal_count = len(list(animal.glob(\"*\")))\n",
    "    animal_name = animal.name\n",
    "    print(f\"Number of {animal_name} is {animal_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_h = 250\n",
    "img_w = 250\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    training_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    "    seed = 123,\n",
    "    image_size = (img_h,img_w),\n",
    "    batch_size = batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    training_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    "    seed = 123,\n",
    "    image_size = (img_h,img_w),\n",
    "    batch_size = batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(list(class_names))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_agumentor = Sequential([\n",
    "    # Sharpen(),\n",
    "    layers.RandomCrop(height=225,width=225,input_shape=(img_h,img_w,3)),\n",
    "    layers.RandomFlip('horizontal',input_shape=(img_h,img_w,3)),\n",
    "    layers.RandomZoom(0.5),\n",
    "    layers.RandomBrightness(0.3),\n",
    "    layers.RandomFlip('vertical',input_shape=(img_h,img_w,3)),\n",
    "    layers.RandomContrast(0.7),\n",
    "    layers.RandomRotation(0.1),\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for images,_ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        aug_imgs = data_agumentor(images)\n",
    "        ax = plt.subplot(3,3,i+1)\n",
    "        plt.imshow(aug_imgs[0].numpy().astype('uint8'))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  data_agumentor,\n",
    "  layers.Rescaling(1./255),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.AveragePooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(24, activation='relu'),\n",
    "  layers.Dropout(0.3),\n",
    "  layers.Dense(num_classes, name=\"outputs\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer = Adam(learning_rate=0.00075), \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopAtThreshold(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, monitor='val_accuracy', threshold=0.90):\n",
    "        super(StopAtThreshold, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None:\n",
    "            if current >= self.threshold:\n",
    "                print(f\"\\nEpoch {epoch}: Reached {self.monitor} of {current}, stopping training.\")\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "val_acc = []\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "epoch_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "loss_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "hi_acc_stopping = StopAtThreshold(monitor='val_accuracy',threshold=0.9)\n",
    "\n",
    "slowed_count = 0\n",
    "regres = False\n",
    "n = 5\n",
    "alpha = 0.0004\n",
    "full_process = False\n",
    "for i in range(1,n+1):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    tf.keras.backend.set_value(model.optimizer.learning_rate, alpha)\n",
    "    \n",
    "    print(\"Phase:\",i,\"alpha:\",float(model.optimizer.learning_rate))\n",
    "    epochs = 10\n",
    "    training_record = model.fit(\n",
    "        train_ds,\n",
    "        validation_data = val_ds,\n",
    "        epochs = epochs,\n",
    "        callbacks=[hi_acc_stopping]\n",
    "    )\n",
    "\n",
    "    train_acc += training_record.history['accuracy'] # getting training and validation accuracy\n",
    "    val_acc += training_record.history['val_accuracy']\n",
    "\n",
    "    train_loss += training_record.history['loss']\n",
    "    val_loss += training_record.history['val_loss'] # getting training and validation loss\n",
    "\n",
    "    last_acc = val_acc[-1]\n",
    "    last_loss = val_loss[-1]\n",
    "    \n",
    "    #if len(train_loss)%20 == 0: alpha*= 0.75\n",
    "    \n",
    "    loss_diff = 0\n",
    "    for i in range(epochs): loss_diff += train_loss[-(i+1)]-val_loss[-(i+1)]\n",
    "    \n",
    "    if loss_diff < 0.05: print(\"[WARNING] Learning Slowed!\"); slowed_count += 1\n",
    "    \n",
    "    gradient = 0 # detecting learning greation. Also happens with overfitting.\n",
    "    for i in range(0,epochs,2): gradient += (val_acc[i+1] - val_acc[i])/2\n",
    "    gradient = gradient/(epochs//2)\n",
    "    if gradient < 0 and regres:\n",
    "        break\n",
    "    elif gradient < -0.1: \n",
    "        print(\"[WARNING] Learning Regression!\")\n",
    "        regres = True\n",
    "    elif gradient > 0:\n",
    "        regres = False \n",
    "    \n",
    "    if model.stop_training:break\n",
    "\n",
    "full_process = True\n",
    "print(\"Learing Slowed\",slowed_count,\"time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not full_process:\n",
    "    train_acc += training_record.history['accuracy'] # getting training and validation accuracy\n",
    "    val_acc += training_record.history['val_accuracy']\n",
    "\n",
    "    train_loss += training_record.history['loss']\n",
    "    val_loss += training_record.history['val_loss'] # getting training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def smoothen(lst,sample_rate=0.05):\n",
    "    if len(lst) == 0: return []\n",
    "    rlist = []\n",
    "    area = math.ceil(sample_rate * len(lst))\n",
    "    for upperbound in range(len(lst)):\n",
    "        lowerbound = max(0,upperbound-area)\n",
    "        # print(lowerbound,upperbound)\n",
    "        sample = lst[lowerbound:upperbound+1].copy()\n",
    "        sample.sort()\n",
    "        median = math.ceil(0.5*len(sample))\n",
    "        rlist.append(sample[median-1])\n",
    "    return rlist\n",
    "\n",
    "def gradient(lst,sampling=True,sample_rate=0.25):\n",
    "    if len(lst)==0:return []\n",
    "    rlist = []\n",
    "    area = math.ceil(sample_rate* len(lst))\n",
    "    if not sampling: area = 2\n",
    "    for upperbound in range(len(lst)):\n",
    "        lowerbound = max(0,upperbound - area)\n",
    "        sample = lst[lowerbound:upperbound+1].copy()\n",
    "        gradients = [(sample[i+1] - sample[i])/2 for i in range(0,len(sample)-1,2)]\n",
    "        if len(sample) == 1: gradients = [0]\n",
    "        gradients.sort()\n",
    "        mid = math.ceil(0.5 * len(gradients))\n",
    "        \n",
    "        rlist.append(gradients[mid - 1])\n",
    "    return rlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(len(train_loss)) # getting a range for the epochs to act as a time scale.\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(epoch_range,train_acc,color='lightblue',alpha= 0.9,label='Training Accuracy')\n",
    "plt.plot(epoch_range,val_acc,color='pink',alpha= 0.8,label='Validation Accuracy')\n",
    "plt.plot(epoch_range,smoothen(train_acc),color='blue',label='Training Accuracy Median')\n",
    "plt.plot(epoch_range,smoothen(val_acc),color='red',label='Validation Accuracy Median')\n",
    "plt.plot(epoch_range,[max(val_acc) for _ in range(len(epoch_range))],color='orange',label='Max Val Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracies')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(epoch_range,train_loss,color='lightblue',alpha= 0.9,label='Training Loss')\n",
    "plt.plot(epoch_range,val_loss,color='pink',alpha= 0.8,label='Validation Loss')\n",
    "plt.plot(epoch_range,smoothen(train_loss),color='blue',label='Training Loss Median')\n",
    "plt.plot(epoch_range,smoothen(val_loss),color='red',label='Validation Loss Median')\n",
    "plt.plot(epoch_range,[min(val_loss) for _ in range(len(epoch_range))],color='orange',label='Min Val Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracies = {}\n",
    "Var_av = {}\n",
    "\n",
    "for animal_dir in list(testing_dir.iterdir()):\n",
    "    class_count = 0\n",
    "    correct_answers = 0\n",
    "    scores = []\n",
    "    for img_path in list(animal_dir.iterdir()):\n",
    "        try:\n",
    "            img = tf.keras.utils.load_img(\n",
    "                img_path, target_size=(img_h, img_w)\n",
    "            )\n",
    "            class_count += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        img_array = tf.keras.utils.img_to_array(img)\n",
    "        img_array = tf.expand_dims(img_array,0)\n",
    "\n",
    "        predictions = model.predict(img_array)\n",
    "        score = tf.nn.softmax(predictions[0])\n",
    "        scores.append(tuple(score.numpy()))\n",
    "        \n",
    "        if animal_dir.name == class_names[np.argmax(score)]: correct_answers += 1\n",
    "    Accuracies[animal_dir.name] = correct_answers/class_count\n",
    "    avgs = [sum(element[i] for element in scores)/class_count for i in range(num_classes)]\n",
    "    mean_avgs = sum(avgs)/len(avgs)\n",
    "    Var_av[animal_dir.name] = sum([pow(avgs[i] - mean_avgs,2) for i in range(len(avgs))]) #this migth not be saying what i want it to be saying...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Accuracy:\")\n",
    "for key in Accuracies.keys():\n",
    "    print(\"<\"+key+\">\",Accuracies[key])\n",
    "\n",
    "print()\n",
    "print(\"Variance in Average Confidence\")\n",
    "for key in Accuracies.keys():\n",
    "    print(key,\"had\",Var_av[key])\n",
    "\n",
    "# bird scored 0.8571428571428571\n",
    "# cat scored 0.8095238095238095\n",
    "# dog scored 0.42857142857142855\n",
    "# snake scored 0.9523809523809523"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "saved_file_name = project_folder + \"_model_v\"\n",
    "version_count = 1\n",
    "for dir in list(pathlib.Path(current_path + \"\\\\models\").glob(\"*.tflite\")):\n",
    "    if dir.name.startswith(saved_file_name): version_count += 1\n",
    "saved_file_name += str(version_count)\n",
    "\n",
    "# Save the model.\n",
    "with open('models\\\\'+saved_file_name+\".tflite\", 'wb') as f:\n",
    "  f.write(tflite_model)\n",
    "\n",
    "print(\"File saved as\",saved_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
